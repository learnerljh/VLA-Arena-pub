# Fine-tuning and Evaluation Guide Using VLA-Arena Generated Datasets

VLA-Arena provides a complete framework for collecting data, converting data formats, and evaluating vision-language-action models. This guide will walk you through how to fine-tune and evaluate various VLA models using datasets generated by VLA-Arena. We currently support fine-tuning and evaluation for OpenVLA, OpenVLA-OFT, Openpi, UniVLA, and SmolVLA models.

## General Models (OpenVLA, OpenVLA-OFT, UniVLA, SmolVLA)

For models other than Openpi (OpenVLA, OpenVLA-OFT, UniVLA, SmolVLA), the usage is very straightforward:

### Install Dependencies

First, install the dependencies for the corresponding model:

```bash
conda create -n [model_name]_vla_arena python==3.10 -y
pip install -e .
pip install vla-arena[model_name]
```

Examples:
- OpenVLA: `pip install vla-arena[openvla]`
- OpenVLA-OFT: `pip install vla-arena[openvla-oft]`
- UniVLA: `pip install vla-arena[univla]`
- SmolVLA: `pip install vla-arena[smolvla]`

### Fine-tune Model

Use the following command to fine-tune:

```bash
vla-arena train --model <model_name> --config <config_file_path>
```

Example:
```bash
vla-arena train --model openvla --config /vla_arena/config/openvla.yaml
```

### Evaluate Model

Use the following command to evaluate:

```bash
vla-arena eval --model <model_name> --config <config_file_path>
```

Example:
```bash
vla-arena eval --model openvla --config /path/to/config.yaml
```

---

## Openpi

The Openpi model requires using `uv` for environment management, and the steps are slightly different from other models.

### Environment Setup

1. Create a new environment and navigate to the Openpi directory:

```bash
conda create -n openpi python=3.11 -y
conda activate openpi
pip install uv
uv pip install -e .
cd vla_arena/models/openpi
```

2. Use uv to sync dependencies and install:

```bash
uv sync
uv pip install -e .
```

### Define Training Configuration and Run Training

Before running training, we need to compute normalization statistics for the training data. Run the following script with your training configuration name. Training configurations can be adjusted in `src/openpi/training/config`:

```bash
uv run scripts/compute_norm_stats.py --config-name <CONFIG_NAME>
```

**Note**: We provide functionality to reload state/action normalization statistics from pretraining. This can be beneficial if you are fine-tuning on a new task with a robot included in the pretraining mixed dataset. For more detailed information on how to reload normalization statistics, please refer to the `docs/norm_stats.md` file.

Now we can start training (the `--overwrite` flag is used to overwrite existing checkpoints when you rerun fine-tuning with the same configuration):

```bash
XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run trainer.py --config <config_file_path>
```

This command will log training progress to the console and save checkpoints to the `checkpoints` directory. You can also monitor training progress on the Weights & Biases dashboard. To maximize GPU memory usage, set `XLA_PYTHON_CLIENT_MEM_FRACTION=0.9` before running trainingâ€”this allows JAX to use up to 90% of GPU memory (the default is 75%).

### Start Policy Server and Run Inference

After training is complete, we can run inference by starting a policy server and then querying it from an evaluation script. Starting the model server is straightforward (this example uses the checkpoint from iteration 20,000, please modify as needed):

```bash
uv run scripts/serve_policy.py policy:checkpoint --policy.config=<CONFIG_NAME> --policy.dir=checkpoints/pi05_libero/my_experiment/20000
```

This will start a server listening on port 8000, waiting for observation data to be sent to it. Then we can run an evaluation script (or robot runtime) to query the server.
If you want to embed policy server calls in your own robot runtime, we provide a minimal example in the remote inference documentation.

### Evaluate Model

After starting the policy server, run the following in the openpi directory:

```bash
uv run evaluator.py --config <config_file_path>
```

---

## Configuration File Notes

Configuration files typically contain information such as dataset paths, model parameters, training hyperparameters, etc. Please refer to the corresponding configuration examples based on the model type you are using.
